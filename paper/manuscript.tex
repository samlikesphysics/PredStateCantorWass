%%
%% This is file `sample-lualatex.tex', % generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed % with new filenames
%distinct from sample-lualatex.tex.
%% 
%% For distribution of the original source see the terms % for copying and
%modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the % original source
%files, as listed above, are part of the % same distribution. (The sources need
%not necessarily be % in the same archive or directory.)
%%
%%
%% Commands for TeXCount TC:macro \cite [option:text,text] TC:macro \citep
%[option:text,text] TC:macro \citet [option:text,text] TC:envir table 0 1
%TC:envir table* 0 1 TC:envir tabular [ignore] word TC:envir displaymath 0 word
%TC:envir math 0 word TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf, anonymous, review]{acmart}
\usepackage{probs}
\usepackage[linesnumbered,ruled]{algorithm2e}
\mathchardef\mhyphen="2D
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you % when you
%complete the rights form.  These commands have SAMPLE % values in them; it is
%your responsibility as an author to replace % the commands and values with
%those provided to you when you % complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
%\acmDOI{10.1145/1122445.1122456}
\acmConference[KDD'22]{the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining}{August 2022}{Washington, DC, USA}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural Gaze
%Detection}{June 03--05, 2018}{Woodstock, NY} \acmBooktitle{Woodstock '18: ACM
%Symposium on Neural Gaze Detection, June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00} \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID. % Use this when submitting an article to a sponsored event.
%You'll % receive a unique submission ID from the organizers % of the event, and
%this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and % references.
%The command \citestyle{authoryear} switches to the % "author year" style.
%%
%% If you are preparing content for an event % sponsored by ACM SIGGRAPH, you
%must use the "author year" style of % citations and references. % Uncommenting
%% the next command will enable that style. %\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter, % allowing the author to
%define a "short title" to be used in page headers.
\title{Predictive State Geometry via Cantor Embeddings and Wasserstein Distance}
%%
%% The "author" command and its associated commands are used to define % the
%authors and their affiliations. % Of note is the shared affiliation of the
%first two authors, and the % "authornote" and "authornotemark" commands % used
%to denote shared contribution to the research.
\author{Samuel P. Loomis}
\email{sloomis@ucdavis.edu}
\orcid{0000-0003-1587-9874}

\author{James P. Crutchfield}
\email{chaos@ucdavis.edu}
\orcid{0000-0003-4466-5410}
\affiliation{%
  \institution{Complexity Sciences Center and Physics Department, \\University of California at Davis}
  \streetaddress{One Shields Ave.}
  \city{Davis}
  \state{CA}
  \country{USA}
  \postcode{95616}
}

%%
%% By default, the full list of authors will be used in the page % headers.
%Often, this list is too long, and will overlap % other information printed in
%the page headers. This command allows % the author to define a more concise
%list % of authors' names for this purpose.
\renewcommand{\shortauthors}{Loomis and Crutchfield}

%%
%% The abstract is a short summary of the work to be presented in the % article.
\begin{abstract}
  Predictive states for stochastic processes are a non-parametric and
  interpretable construct with relevance across a multitude of modeling
  paradigms. Recent progress on the self-supervised reconstruction of predictive
  states from time-series data has focused on the use of reproducing kernel
  Hilbert spaces. Here, we examine how Wasserstein distances may be used to
  detect predictive equivalences in symbolic data. We construct a
  finite-dimensional embedding of sequences based on the Cantor set, and use
  distances in this embedding to compute Wasserstein distances between
  distributions over sequences (``predictions''). We show that analyzing the
  resulting geometry (via hierarchical clustering and dimension reduction)
  provides insight into the temporal structure of processes ranging from the
  relatively simple (\emph{e.g.}\ hidden Markov models) to the very complex
  (\emph{e.g.}\ indexed grammars).
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %
%Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10002950.10003648.10003649</concept_id>
  <concept_desc>Mathematics of computing~Probabilistic representations</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10002950.10003648.10003649.10003657</concept_id>
  <concept_desc>Mathematics of computing~Nonparametric representations</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10002950.10003648.10003700.10003701</concept_id>
  <concept_desc>Mathematics of computing~Markov processes</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10002950.10003648.10003700</concept_id>
  <concept_desc>Mathematics of computing~Stochastic processes</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10002950.10003648.10003688.10003693</concept_id>
  <concept_desc>Mathematics of computing~Time series analysis</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10002950.10003741.10003742</concept_id>
  <concept_desc>Mathematics of computing~Topology</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10002950.10003712</concept_id>
  <concept_desc>Mathematics of computing~Information theory</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003766</concept_id>
  <concept_desc>Theory of computation~Formal languages and automata theory</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003766.10003771</concept_id>
  <concept_desc>Theory of computation~Grammars and context-free languages</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Probabilistic representations}
\ccsdesc[500]{Mathematics of computing~Nonparametric representations}
\ccsdesc[500]{Mathematics of computing~Stochastic processes}
\ccsdesc[500]{Mathematics of computing~Time series analysis}
\ccsdesc[300]{Mathematics of computing~Markov processes}
\ccsdesc[300]{Mathematics of computing~Topology}
\ccsdesc[300]{Mathematics of computing~Information theory}
\ccsdesc[300]{Theory of computation~Formal languages and automata theory}
\ccsdesc[300]{Theory of computation~Grammars and context-free languages}
    
%%
%% Keywords. The author(s) should pick words that accurately describe % the work
%being presented. Separate the keywords with commas.
\keywords{time series, predictive states, wasserstein distance, hierarchical clustering}

%% A "teaser" image appears between the author and affiliation % information and
%the body of the document, and typically spans the % page.

%%
%% This command processes the author and affiliation and title % information and
%builds the first part of the formatted document.
\maketitle

\section{Introduction}
Suppose that we have a finite sequence $x_1 \dots x_L$ of categorical
observations drawn from a temporal process. We may suppose that the process is
stationary (time-translation-invariant) and ergodic (has a tendency to explore
all possible behaviors) \cite{Kall01a,Shal07a}. We may wish to forecast from the
observed information the behavior of the next $n$ observations. If we suspect
that temporal correlations of the process do not matter much beyond $k$ symbols,
then we can use the data we have to take all subsequences of length $n+k$,
splitting each subsequence into words of length $k$ and $n$ respectively. From
these ``past/future'' pairs we could construct an empirical conditional
distribution
\begin{equation}\label{eq:conditional}
  \hat{P}_{x_{1}\dots x_{n}|x_{-k+1}\dots x_0} = 
  \frac{C_{x_{-k+1}\dots x_{n}}}{C_{x_{-k+1}\dots x_0}}
\end{equation}
where $C_{w}$ is the number of times the word $w$ appears in our sequence $x_1
\dots x_L$. 

But suppose now that we do not know how long-range the temporal dependencies in
our process can stretch. Even very simple stochastic processes can have infinite
Markov order, indicating potential long-term dependence of future observations
on the past \cite{Uppe97a}. Given sufficient data, it would be desirable to take
pasts of arbitrary length, and converge towards some prediction conditioned on
the \emph{infinite} past:
\begin{equation}\label{eq:convergence}
  {P}_{x_{1}\dots x_{n}|\overleftarrow{x}}
  = \lim_{k\rightarrow\infty}
  \hat{P}_{x_{1}\dots x_{n}|x_{-k}\dots x_0}
\end{equation}
where $\overleftarrow{x}$ denotes an infinite sequence $\overleftarrow{x} =
(\dots,x_{-1},x_0)$ of observations stretching into the past. This mathematical
ideal is known to previous literature as the \emph{causal} or \emph{predictive
state} \cite{Crut88a,Jaeg00a}. Formally, the conditional predictions
${P}_{x_{1}\dots x_{n}|\overleftarrow{x}}$ for all forecast lengths $n$ together
describe a \emph{probability measure} over future sequences $\overrightarrow{x}
= (x_1,x_2,\dots)$; the predictive state is this measure. We will denote it
simply by $P_{\overleftarrow{x}}$.

Predictive states have been utilized for inference and modeling in dynamical
systems \cite{Crut92c,Emen16a,Salo19a}, renewal processes and spike-trains
\cite{Marz14e,Marz15a}, condensed matter physics \cite{Varn12a,Varn14a} and
spatiotemporal systems \cite{Rupe19a,Rupe19b,Rupe20a}, and a deep mathematical
theory of predictive state inference has been correspondingly developed
\cite{Jaeg00a,Shal02a,Jame04a,Stil07b,Stre13a,Thon15a,Marz19a,Brod20a}. Despite
this, universal conditions for predictive state convergence were not given until
recently. It is now known that if the dataset $x_1 \dots x_L$ is drawn from any
stationary process, and if $L$ is sufficiently large, then Eq.
\eqref{eq:convergence} will converge for any $n$ and a probability-1 subset of
pasts $\overleftarrow{x}$ \cite{Loom21a}. In the language of measures,
$P_{\overleftarrow{x}}$ converges \emph{in distribution}, with respect to the
\emph{product topology} of the space of sequences $\mathcal{X}^\mathbb{N}$.

The general goals of predictive state analysis are typically threefold
\cite{Shal01a}. The first is to understand the overall structure of how the
predictive states relate to one another geometrically, and possibly use this
geometry to classify pasts based on equivalence of their predictive states. The
second is to actually reproduce the prediction to a specified accuracy. The
third is to understand the dynamics of how predictions evolve under a stream of
new observations. We will focus in this paper on the first, as it is a crucial
building block to achieving the other two.

Recent attempts at reconstructing the geometry of predictive states have focused
on embedding them in reproducing kernel Hilbert spaces
\cite{Song09a,Song10a,Boot13a,Brod20a,Loom21a}, to great effect largely because
convergence in Hilbert spaces generated by universal kernels is equivalent to
convergence in distribution \cite{Srip10a}. That is, these embeddings allow
accurate representations of predictive states because they respect the product
topology of sequences in the same way that predictive states themselves do. 

Understanding the source of the power of RKHS methods in predictive state
analysis frees us to consider other options. In this paper, we will embed
symbolic sequences in a one-dimensional space that has the same topology as the
product space. This embedding is inspired by the fractal Cantor set
\cite{Kurk03a}. Predictive states can then be thought of as distributions in
this one-dimensional space. We then use the Wasserstein distance to compute the
geometry between predictive states, which can be computed with a closed-form
integral for one-dimensional distributions. This works as an alternative to
RKHS-based distances because the Wasserstein distance also reproduces the
topology of convergence in distribution \cite{Pane19a}. The resulting distance
matrix can be used to find low-dimensional embeddings \cite{Borg05a} of the
geometry or hierarchical clusterings \cite{Mull11a} of the predictive states.
The latter, in particular, when combined with our fractal embedding, provides a
highly interpretable visualization of the predictive state space.

\section{Example processes}
The methods in this paper are intended to be applied toward stationary and
ergodic stochastic processes which produce categorical time-series data. For our
intents we can think of a stochastic process as a collection of probability
distributions $\Prob{\mu}{x_1\dots x_L}$ over any finite, contiguous sequence,
taking values in a finite set $\mathcal{X}$. Formally this describes a measure
$\mu$ over the set of all bi-infinite sequences $(\dots,x_{-1},x_0,x_1,\dots)\in
\mathcal{X}^{\mathbb{Z}}$. These sorts of processes can be generated by a number
of systems with widely varying complexity. Most popularly studied are those
methods often characterized as having some degree of ``finite memory'': Markov
models, hidden Markov models, and observable operator models (also termed
generalized hidden Markov models) \cite{Uppe97a,Jaeg00a}. Beyond these one can
also generate processes using probabilistic grammars, such as probabilistic
context-free and indexed grammars \cite{Gema00a}. Additionally, coarse-grained
data from dynamical systems (such as the logistic map) can display behavior
varying widely in complexity \cite{Crut92c}. In this paper we will refer back
frequently to the following example processes:
\begin{enumerate}
  \item The \emph{even process} can be generated by repeatedly tossing a coin
  and writing down a $0$ for every tail and $11$ for every head. The process is
  essentially random except for the constraint that $1$'s can only appear in
  contiguous blocks of even size. The even process has infinite Markov order but
  can be generated by a two-state hidden Markov process \cite{Crut01a}.
  \item The $a^n b^n$ \emph{process} can be generated by choosing a random
  integer $n\geq 1$ (we will suppose via a Poisson process) and writing $n$
  $\mathtt{a}$'s followed by an equal number of $\mathtt{b}$'s, and then
  repeating this process indefinitely. The result will be sequences where any
  contiguous block of $\mathtt{a}$'s is followed by a block of $\mathtt{b}$'s of
  equal size. The $\mathtt{a}^n \mathtt{b}^n$ process cannot be modeled by any
  hidden Markov model, though it is a simple example of a probabilistic
  context-free grammar \cite{Hopc06a}.
  \item The $x+f(x)$ \emph{process} is a probabilistic context-free grammar
  modeling simple mathematical expressions. It has terminal symbols
  $\{\mathtt{(}\ ,\ \mathtt{)}\ ,\ \mathtt{;}\ ,\ \mathtt{+}\ ,\ \mathtt{f}\ ,\
  \mathtt{x}\}$ and non-terminals $\{A,B,C\}$, and starts with a sequence osf
  $A$'s with the production rules:
  \begin{align*}
    A &\mapsto B\ \mathtt{+}\ C\ \mathtt{;}\ |\ C\ \mathtt{;}\\
    B &\mapsto B\ \mathtt{+}\ C\ |\ C\\
    C &\mapsto \mathtt{f(}B\mathtt{)}\ |\ \mathtt{x}
  \end{align*}
  \item The $a^n b^n c^n$ \emph{process} is a probabilistic indexed grammar
  \cite{Hopc06a} which is analagous to $\mathtt{a}^n \mathtt{b}^n$ except after
  writing the blocks of $\mathtt{a}$'s and $\mathtt{b}$'s, we also write a block
  of $\mathtt{c}$'s of length $n$.
  \item The \emph{Feigenbaum process} is generated by sampling from the time
  series of the logistic map at critical parameter $r\approx 3.56995$:
  \begin{equation*}
    y_{t+1} = r y_t(1-y_t)
  \end{equation*}
  and then coarse-graining the data by taking $x_t = 0$ if $0< y_t \leq
  \frac{1}{2}$ and $x_t = 1$ if $\frac{1}{2} < y_t <1$ \cite{Kurk03a}.
  Alternatively, we can generate this process by starting with a single $0$ and
  executing the replacements $0\mapsto 11$ and $1 \mapsto 01$ consecutively. The
  result of the Feigenbaum process is indexed-context free \cite{Crut92c}.
\end{enumerate}

\section{Cantor-embedding sequences}
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.4\linewidth]{../plots/even_CANTOR.png}
  \includegraphics[width=0.4\linewidth]{../plots/anbn_CANTOR.png}
  \includegraphics[width=0.4\linewidth]{../plots/anbncn_CANTOR.png}
  \includegraphics[width=0.4\linewidth]{../plots/x+f(x)_CANTOR.png}
  \caption{ Cantor plots for the even, $\mathtt{a}^n\mathtt{b}^n$,
    $\mathtt{a}^n\mathtt{b}^n\mathtt{c}^n$, and $\mathtt{x+f(x)}$ processes.
    Each point $(x,y)$ corresponds to a pair of sequences corresponding to the
    past and future respectively. The symbol on the $x$ ($y$) axis indicate that
    all points above (to the right of) that symbol have a past (future) whose
    most recent observation is that symbol. Though not marked, further
    proportional subdivisions of each segment of the axes would indicate the
    value of the second, third, \emph{etc.} symbols. For instance, one can read
    from the $\mathtt{x+f(x)}$ fractal that any past which ends in $\mathtt{f}$
    must be paired with a future beginning in $\mathtt{(f}$ or $\mathtt{(x}$. }
    \label{fig:cantor}
  \Description{Four fractals are displayed, respectively labeled as the even,
  $\mathtt{a}^n\mathtt{b}^n$, $\mathtt{a}^n\mathtt{b}^n\mathtt{c}^n$, and
  $\mathtt{x+f(x)}$ processes.}
\end{figure*}

The geometry of sequences is inherently self-similar. Given an infinite sequence
$\overrightarrow{x} = (x_1,x_2,\dots)$, we can split it into its leading word
$x_1 x_2 \dots x_L$ and a following sequence $\overrightarrow{x}_L =
(x_L,x_{L+1},\dots)$. That is, the space of sequences $\mathcal{X}^\mathbb{N}$
can be factored into $\mathcal{X}^L\times \mathcal{X}^\mathbb{N}$ for any $L$.
The fractal nature of sequence-space is encoded in the structure of its
\emph{product topology}.

We can exploit this self-similarity in an interesting way by constructing a
mapping between sequence space and the celebrated Cantor set (or one of its
generalizations). Suppose a symbolic sequence $(x_1,x_2,\dots)$ takes values in
an alphabet $\mathcal{X}$ of size $|\mathcal{X}|$. To each $x\in\mathcal{X}$ we
can associate some unique integer between $0$ and $|\mathcal{X}|-1$ inclusive;
call this $J(x)$. Then there is a function $C:\mathcal{X}^\mathbb{N}\rightarrow
[0,1]$ which maps every sequence to a positive real number:
\begin{equation}
  C(x_1,x_2,\dots) = \sum_{k=1}^\infty \frac{2J(x_k)}{(2|\mathcal{X}|-1)^k}
\end{equation}
For instance, suppose that $|\mathcal{X}|=2$ has two elements; then $C$ maps the
sequence to a point the traditional Cantor set fractal. For a finite sequence of
length $L$, truncate the sum at $k=L$.

Remarkably, the embedding $C$ has the property that for any continuous function
$f$ on $[0,1]$, the function $F(\overrightarrow{x}) = f(C(\overrightarrow{x}))$
is continuous on $\mathcal{X}^\mathbb{N}$; further, if $F$ is continuous on
$\mathcal{X}^\mathbb{N}$, then $f(y) = F(C^{-1}(y))$ is continuous on the image.
Thus the embedding $C$ respects the basic structure of the product topology
\cite{Kurk03a}.

Stationary processes, due to their time-translation invariance, inherit the
fractal temporality of sequence space. This can be neatly visualized: given a
length-$L$ sample $x_1\dots x_L$, and some $n,k>0$, take a sliding window of
pasts and futures, $(x_{t-n+1}\dots x_t, x_{t+1}\dots x_k)$ for $t=n,\dots,L-k$.
For each past-future pair, compute the truncated Cantor embeddings on the
\emph{reversed} past and (unreversed) future: $\left(C(x_t \dots
x_{t-n+1}),C(x_{t+1}\dots x_k)\right)$. The resulting pairs of real numbers can
be plotted as $(x,y)$-values on a scatter plot. The fractal which emerges
contains, in essence, all information necessary to understand the temporal
structures of the process. See Fig. \ref{fig:cantor} for examples and guidance
on how to interpret the visualization.

It is worth noting that for $|\mathcal{X}|>2$, the embedding $C$ also introduces
additional structure which may or may not be desirable. The association of each
symbol $x$ with an integer $j_x$ endows an ordinal structure on the set
$\mathcal{X}$; this ordinality is present in the macroscopic geometry of
$C\left(\mathcal{X}^{\mathbb{N}}\right)$. Later we will examine higher-dimension
embeddings which do not have this ordinal aspect; however, they will come at the
cost of increased computational complexity for the Wasserstein distance
computation, so for now we will assume that ordinal artifacts are either desired
or sufficiently tolerable as to not outweight the computational benefits of
working in one dimension.

\section{Wasserstein distance on predictive states}

\begin{algorithm}[t]
  \SetKwData{UnqPasts}{UnqPasts}\SetKwData{Cantors}{Cantors}\SetKwData{Wass}{Wass}
  \SetKwFunction{Wasserstein}{Wasserstein}\SetKwFunction{Append}{append}
  \SetKwFunction{Index}{index} \SetKwFunction{Length}{length}
  \SetKwFunction{Matrix}{Matrix}
  \SetKwFunction{CantorWasserstein}{CantorWasserstein}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  
  \underline{function \CantorWasserstein} ($n,k,x_1\dots x_L$)\;
  \Input{Integers $n,k$ of past and future lengths} 
  \Input{Length-$L$ sequence $x_1\dots x_L$ of observations} 
  \Output{List $\UnqPasts$ of unique pasts}
  \Output{List of lists $\Cantors$ of Cantor-embedded futures} 
  \Output{Matrix $\Wass$ of Wasserstein distances} 
  $\UnqPasts \leftarrow \left[\right]$\; 
  $\Cantors \leftarrow \left[\right]$\; 
  \For{$t\leftarrow n$ \KwTo $L-k$} { 
    $\overleftarrow{\mathbf{x}} \leftarrow \left[x_{t-n+1},\dots, x_{t}\right]$\;
    $\overrightarrow{\mathbf{x}} \leftarrow \left[x_{t+1},\dots, x_{t+k}\right]$\;
      \vspace{-1.5em}
      \begin{flalign*}
        p &\leftarrow \sum_{\ell=1}^L
        \frac{2J(\overrightarrow{x}_\ell)}{(2|\mathcal{X}|-1)^\ell}\ ; &&
      \end{flalign*}

      \eIf{$\overleftarrow{\mathbf{x}} \in \UnqPasts$} { 
        $\Append\ \overleftarrow{\mathbf{x}}$ \KwTo $\UnqPasts$\; 
        $\Append\ [p]$ \KwTo $\Cantors$\; 
        } { 
        $j \leftarrow \Index\left(\overleftarrow{\mathbf{x}},\UnqPasts\right)$\;
        $\Append\ p$ \KwTo $\Cantors_j$\; 
        } 
  }
  $K \leftarrow \Length(\UnqPasts)$\; 
  $\Wass \leftarrow \Matrix(K,K)$\;
  \For{$k\leftarrow 1$ \KwTo $K$} { 
    \For{$\ell\leftarrow 1$ \KwTo $K$} {  
      $\Wass_{k \ell}
        \leftarrow
        \Wasserstein(\Cantors_k,\Cantors_\ell)$\;
        $\Wass_{\ell k} \leftarrow \Wass_{k \ell}$\; } } 
  \KwResult{$\UnqPasts$,$\Cantors$,$\Wass$}
  \caption{Steps for converting a sequence of categorical time-series data
        into a labeled collection of empirical distributions of Cantor-embedded futures,
        and a matrix of Wasserstein distances from said distributions.}
  \label{alg:cantorwass}
\end{algorithm}

The Cantor fractals in Fig. \ref{fig:cantor} can be thought of as probability
distributions. We can consequently interpret a vertical slices of the fractal,
located at horizontal position $C(\overleftarrow{x})$, as visualizing the
predictive state $P_{\overleftarrow{x}}$ as a distribution over Cantor-embedded
futures, $C(\overrightarrow{x})$.

For example, by analyzing the Cantor fractal of the even process, one may notice
that there are effectively only 2 distinct predictive states---every vertical
column is just one of two types. This corresponds with the 2 states of the
hidden Markov model which generates the even process.

This visualization allows us to see how predictive states distribute their
probability over the intrinsic geometry of potential futures. We can compare
predictive states not only on how much their supports overlap, but on how
geometrically close their supports are to one another. For the $\mathtt{a}^n
\mathtt{b}^n$ process, for example, we can see that the first few columns
(corresponding to pasts of the form $\dots \mathtt{b a}^n$ for some $n$) are
inherently similar to one another, though they are shifted upwards the closer to
the axis they are (which corresponds to the increasing number of $\mathtt{b}$'s
in the predicted future as $n$ increases).

The intuitive distance metric between probability measures for capturing this
underlying geometry is the Wasserstein metric \cite{Pane19a}. Given two measures
$\mu$, $\nu$ defined on a metric space $\mathcal{M}$ with metric $d$, the
Wasserstein distance between $\mu$ and $\nu$ is given by
\begin{equation*}
  W(\mu,\nu) = \min_{\pi\in \Gamma(\mu,\nu)}
   \int_{\mathcal{M}\times\mathcal{M}} 
   d(x,y) d\pi(x,y)
\end{equation*}
where $\Gamma(\mu,\nu)$ is the set of all measures on
$\mathcal{M}\times\mathcal{M}$ whose left and right marginals are $\mu$ and
$\nu$ respectively. This can be interpreted as the minimal cost for ``shifting''
the probability mass from one distribution to match shape of the other. 

$W(\mu,\nu)$ the solution to a constrained linear optimization. As a function of
distributions, $W(\mu,\nu)$ is continuous with respect to convergence in
distribution; in fact, convergence under the Wasserstein distance is equivalent
to convergence in distribution on compact spaces. This makes $W(\mu,\nu)$ ideal
for measuring geometry between predictive states, since empirical estimates of
these are known to converge in distribution.

When $\mathcal{M} \subseteq \mathbb{R}$, there is in fact a closed-form solution
to the Wasserstein optimization problem \cite{Thas10a}. Let $F$ and $G$
respectively be the cumulative distributions functions of $\mu$ and $\nu$. Then
\begin{equation}
  W(\mu,\nu) = \int_{-\infty}^\infty \left|F(t)-G(t)\right| dt
\end{equation}
This closed-form solution is considerably faster to compute than the linear
optimization required for arbitrary metric spaces. Because the Cantor embedding
embeds the space of sequences directly into $[0,1]$, we can utilize this
formula.

The combination of the Cantor embedding and the Wasserstein distance lays out a
fairly straightforward programme for analyizing a categorical time series:
\begin{enumerate}
  \item Apply Algorithm \ref{alg:cantorwass} to the data stream $x_1\dots x_L$
  for a specified past length $k$ and future length $n$, retrieving the set of
  unique observed pasts, the empirical Cantor distributions corresponding to
  each past, and the matrix of Wasserstein distances computed from these
  distributions.
  \item Use the Wasserstein distance matrix to perform additional methods of
  geometric data analysis, such as hierarchical clustering \cite{Mull11a} and
  multidimensional scaling \cite{Borg05a}, in order to elucidate the relative
  geometry of the predictive states.
\end{enumerate}
In the next two sections we examine the results of this approach.

\section{Interpretable predictive states with hierarchical clustering}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.45\linewidth]{../plots/even_HCLUST.png}
  \includegraphics[width=0.45\linewidth]{../plots/anbn_HCLUST.png}
  \includegraphics[width=0.45\linewidth]{../plots/anbncn_HCLUST.png}
  \includegraphics[width=0.45\linewidth]{../plots/x+f(x)_HCLUST.png}
  \caption{From upper left to lower right, the clustered Cantor diagrams of the even, $\mathtt{a}^n\mathtt{b}^n$,
  $\mathtt{a}^n\mathtt{b}^n\mathtt{c}^n$, and $\mathtt{x+f(x)}$ processes (zoom
  for detail). On
  each diagram, the vertical axis shows all pasts of a given length (length
  $k=8$ for the even, $\mathtt{a}^n\mathtt{b}^n$ and
  $\mathtt{a}^n\mathtt{b}^n\mathtt{c}^n$ processes and $k=4$ for the
  $\mathtt{x+f(x)}$ process), along with their hierarchically clustered
  dendrogram. For the purposes of this graphic the coloring threshold was
  chosen to aid in visual interpretation. The lines in each row show the
  empirical distribution of Cantor-embedded futures observed following each
  past. As such the horizontal axis corresponds exactly to the vertical axis of
  Fig. \ref{fig:cantor}.}
    \label{fig:hclust}
  \Description{Four graphs are displayed. Each vertical axis has a list of symbolic sequences
  organized by a hierarchical clustering tree. The horizontal axis is a list of
  individual symbols. The content of each graph is rows of fractally-distributed
  lines.}
\end{figure*}
In Fig. \ref{fig:hclust} we display the result of collecting the Cantor-embedded
empirical predictions for all pasts of a given length, for four processes (even,
$\mathtt{a}^n \mathtt{b}^n$, $\mathtt{a}^n \mathtt{b}^n \mathtt{c}^n$ and
$\mathtt{x+f(x)}$). In each case the Wasserstein distance between every pair of
predictions was computed and used to hierarchically cluster the pasts (using the
Ward method \cite{Mull11a}) with
others that produced similar predictions.

The resulting clustered Cantor plots offer a highly interpretable visualization
of the relationship between pasts and futures, and of the geometry of the
predictive states. They are, in a certain sense, a sorting of the columns in the
Cantor fractals of Fig. \ref{fig:cantor}, with the whitespace between columns
removed. For instance, the clustered Cantor plot of the Even process clearly
contains the two major states, with a third ``transient'' state visible
(corresponding to the increasingly unlikely event of never seeing a $0$ in a
block of length $n$). This third state was previously hidden mostly out of view
on the far-right side of the 2-dimensional Cantor plot of the even process in
Fig. \ref{fig:cantor}.

Other features are worth remarking on. Close observation will show the reader
that the hierarchical clustering allows for the (mostly) scale-free distinctions
between pasts with subtle differences. For the $\mathtt{a}^n \mathtt{b}^n$
process, pasts of the form $\dots \mathtt{b a}^n$ are distinguished for
different $n$, as each involves a distinct number of $\mathtt{b}$'s appearing in
the near future; meanwhile, the clustering scheme carefully distinguishes pasts
of the form $\dots \mathtt{b a}^n \mathtt{b}^{n-k}$ for different $k$ but
\emph{not} for different $n$, as $k$ is the essential variable for predicting
the remaining number of $\mathtt{b}$'s. (The scale-free discernment of the
algorithm begins to break down past $n=5$, which is the scale at which sampling
error becomes relevant for our chosen sample size.)

Similar discernment can be noticed for the $\mathtt{a}^n \mathtt{b}^n
\mathtt{c}^n$ and $\mathtt{x+f(x)}$ processes as well. We draw attention to the
manner in which the presence of a semicolon in pasts from $\mathtt{x+f(x)}$
affects the comparison of predictions.

By analyzing clustered Cantor plots, one can gain insight into the properties of
pasts that make them similar in terms of future predictions---even if they are
superficially quite distinct. Furthermore, the horizontal axis allows for
continued use of the natural geometry of the Cantor set for visualizing the
future forecasts associated with each cluster of predictions.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.45\linewidth]{../plots/even_MDS.pdf}
  \includegraphics[width=0.45\linewidth]{../plots/anbn_MDS.pdf}
  \caption{Two scatterplots of the first two MDS coordinates of the
  reconstructed predictive states, for the even and $\mathtt{a}^n\mathtt{b}^n$
  processes respectively. The clusters are colored according to the scheme
  determined by the dendrogram in Fig. \ref{fig:hclust} and the label on each
  cluster describes the pattern which uniquely characterizes the pasts in that cluster.}
    \label{fig:mds}
  \Description{Two scatter plots with colored clusters of points are displayed,
  with labels next to each cluster describing symbolic patterns.}
\end{figure*}

\section{Predictive state geometry with multidimensional scaling}
If we are willing to sacrifice direct visualization of future predictions, we
may gain a more intuitive picture of the geometry of predictive state space. We
can apply any desired dimensional reduction algorithm to the matrix of
Wasserstein distances between predictions and obtain a coordinate representation
of the similarities between predictive states.

In Fig. \ref{fig:mds} we have plotted the first two dimensions of a
multidimensional scaling (MDS) decomposition \cite{Borg05a} for the even process
and the $\mathtt{a}^n\mathtt{b}^n$ process, with clusters colored in the same
manner as in Fig. \ref{fig:hclust} and labeled by the specific pattern which
distinguishes the pasts in some of the clusters. It should be noted that the
clusters and the labels are directly drawn from \ref{fig:hclust} for reference,
and are not a result of the MDS algorithm itself. However, interactive plotting
approaches may allow for similar exploration from these decompositions without
the need for prior clustering.

The even process, as in all other cases we have seen thus far, has two dominant
clusters of predictions, corresponding to the predictive states which result
from seeing an even-sized block of $1$'s (or, equivalently, no $1$'s), and that
resulting from seeing an odd-sized block of $1$'s. The
$\mathtt{a}^n\mathtt{b}^n$ plot is much more sophisticated. Intriguingly, its
geometry not only clearly distinguishes predictively distinct states, but it
organizes them in a manner highly suggestive of an \emph{stack}, particularly
appropriate given that stack automata are the natural analogue of hidden Markov
models for context-free processes. As more $\mathtt{a}$'s are observed, we push
further up the stack towards the upper-right corner of the figure, and as more
$\mathtt{b}$'s are observed we pop out of the stack, back towards the lower left
(representing equality between $\mathtt{a}$'s and $\mathtt{b}$'s).

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{../plots/feig_MDS.pdf}
  \caption{On the left is a scatterplot of the first two MDS coordinates 
  of the reconstructed predictive states for the Feigenbaum process, 
  color-coded by cluster. On the right, we display a scatterplot of the
  corresponding points in the domain of the logistic map, plotting for each
  point both the present value $y_t$ and the next value $y_{t+1}$, with the
  $x=y$ line for reference. Each pair of color-coded arrows shows where each
  cluster maps to under the action of the logistic map. The predictively
  reconstructed clusters thus correspond to dynamically similar neighborhoods of
  the logistic map domain.}
    \label{fig:mds-feig}
  \Description{A rectangular scatter plot of color-coded points is displayed on
  the left, and on the right a square scatterplot of parabolically arranged
  points with a diagonal line cutting through them. In the right plot the points
  are colored with the same scheme as the left, and similarly colored arrows
  point from clusters to the diagonal line or from the diagonal line to the next
  cluster.}
\end{figure*}

The geometric approach is particularly insightful when we compute the
Wasserstein matrix between predictions estimated from Feigenbaum process data.
Recall that the Feigenbaum process is just a coarse-graining of the iterated
logistic map $y_{t+1} = ry_t(1-y_t)$ at the critical parameter $r\approx
3.56995$. The resulting stream of $0$'s and $1$'s is an infamous instance of
high complexity at the ``boundary of order and chaos'', as values of $r$ on
either side tend to result in coarse-grainings which can be generated by hidden
Markov models, but the Feigenbaum process is context-sensitive and therefores
requires several orders higher of model complexity to capture its behavior.

Regardless, we show that the predictive state geometry reconstructed from a
sufficiently large sample of the Feigenbaum process is capable of recovering the
neighborhoods of $[0,1]$ which are relevant to the dynamics of the original
logistic map. What we mean by this is that there is a correspondence between
each past $x_{-n+1}\dots x_{0}$ and a subset $V_{x_{-n+1}\dots x_{0}}$, such
that $V_{x_1\dots x_n}$ is the set of all points $y$ for which $x(f^{-t}(y)) =
x_{-t}$ for $0\leq t < n$ (here $f(y) = ry(1-y)$ and $x(y)$ is the encoding
$y\mapsto 0,1$). As it happens, pasts $x_{-n+1}\dots x_{0}$ whose predictive
states are close under the Wasserstein distance are also pasts for which the
sets $f(V_{x_{-n+1}\dots x_{0}})$ are close (that is, they correspond to
predictively similar ranges of the logistic map variable). 

This relationship between the reconstructed predictive states of the Feigenbaum
process, neighborhoods of the logistic variable $y$, and the logistic map
dynamics is visualized in Fig. \ref{fig:mds-feig}. In short, despite the fact
that the Feigenbaum process is a highly coarse-grained form of the logistic map,
the essential geometry of that map can be recovered by reconstructing predictive
state geometry with the Wasserstein metric and the Cantor embedding.

We should note that, because of the deterministic nature of the Feigenbaum
process, the combination of the Wasserstein metric and the Cantor embedding is
particularly important to achieving this result. Asymptotically, each past
corresponds to a unique future, and so there is no asymptotically no overlap
between predictions. The choice of the Cantor map allows us to place close
together forecasts which match up to a certain time in the future, and the
Wasserstein distance allows us to directly compare predictions whose supports
are geometrically close. It is therefore the combination of these two approaches
which enables the straightforward recovery of the underlying logistic geometry.

\section{Concluding remarks}
We have presented a general approach for predictive state analysis---Cantor
fractal embedding sequences and Wasserstein distance comparison of
predictions---and offered two approaches to visualizing the results of this
method---one a direct application of multidimensional scaling, and the other
being a clustered Cantor diagram built from combining hierarchical clustering
with the introduced Cantor embedding.

In comparison to the use of reproducing kernel Hilbert spaces, which is dominant
approach to predictive states at present
\cite{Song09a,Song10a,Boot13a,Brod20a,Loom21a}, our choice to combine the Cantor
set with the Wasserstein distance may appear rather idiosyncratic, but there are
strong benefits to both methods, and together the two methods synergize their
benefits in a unique way. The topology of convergence in distribution can be
replicated with both the Wasserstein distance and the RKHS inner product, but
the Wasserstein distance depends on far fewer parameters (such as the choice of
the eponymous kernel in RKHS approaches), and its value is directly
interpretable in terms of the shapes of the distributions it compares.

Similarly, there are many ways to metrize the product topology on sequences, but
the Cantor embedding offers a direct way to connect the product topology with a
visualizable geometry, and by embedding in a single dimension enables efficient
computation of the Wasserstein metric. The benefits of the Cantor and
Wasserstein approaches adds interpretability to the resulting predictive state
geometry along two distinct axes, most clearly seen in the clustered Cantor
diagrams of Fig. \ref{fig:hclust}. 

%% The acknowledgments section is defined using the "acks" environment % (and
%NOT an unnumbered section). This ensures the proper % identification of the
%section in the article metadata, and the % consistent spelling of the heading.
\begin{acks}
  We thank Nicolas Brodu, Adam Rupe, Alex Jurgens, David Gier, and Mikhael
  Semaan. JPC acknowledges the kind hospitality of the Telluride Science
  Research Center, Santa Fe Institute, Institute for Advanced Study at the
  University of Amsterdam, and California Institute of Technology for their
  hospitality during visits. This material is based upon work supported by, or
  in part by, Grant Nos. FQXi-RFP-IPW-1902 and FQXi-RFP-1809 from the
  Foundational Questions Institute and Fetzer Franklin Fund (a donor-advised
  fund of Silicon Valley Community Foundation) and grants W911NF-18-1-0028 and
  W911NF-21-1-0048 from the U.S. Army Research Laboratory and the U.S. Army
  Research Office.  
\end{acks}

\section{Reproducibility statement}
For the purposes of reproducibility, we have provided a GitHub repository which
contains the code necessary to generate this paper and its figures, including a
notebook for generating the data we used for our examples and building both
static and interactive figures for further exploration.
%%
%% The next two lines define the bibliography style to be used, and % the
%bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

%%
%% If your work has an appendix, this is the place to put it.

\end{document}
\endinput
%%
%% End of file `sample-lualatex.tex'.
